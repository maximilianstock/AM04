---
title: 'Session 10: Data Science Capstone Project'
author: "Maximilian Stock"
date: "`r Sys.Date()`"
output: 
    html_document:
      number_sections: true
      highlight: haddock
      theme: spacelab
      toc: yes
      toc_depth: 2
      toc_float:
        collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
div.navy1 { background-color:#686868; border-radius: 5px; padding: 20px; border-style: groove; color: #ffffff;}

</style>



```{r, load_libraries, include = FALSE}

if(!is.element("tidyverse", installed.packages()[,1]))
{  install.packages("tidyverse")}

if(!is.element("Hmisc", installed.packages()[,1]))
{  install.packages("Hmisc")} #package for data summary using `describe`

if(!is.element("ggplot2", installed.packages()[,1]))
{  install.packages("ggplot2")} #package for plots
if(!is.element("ggthemes", installed.packages()[,1]))
{  install.packages("ggthemes")} #package to make fancier ggplots

if(!is.element("janitor", installed.packages()[,1]))
{ install.packages("janitor")} #package to visualize results of machine learning tools
if(!is.element("rpart.plot", installed.packages()[,1]))
{  install.packages("rpart.plot")} #package to visualize trees

library(rpart.plot)
library(caret)
library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate)
library(janitor) # clean_names()
library(Hmisc)
library(rsample)
library(scales)
library(GGally)
library(caretEnsemble)
```

# Introduction and learning objectives

<div class = "navy1">
The purpose of this exercise is to build an estimation engine to guide investment decisions in London house market. You will first build machine learning algorithms (and tune them) to estimate the house prices given variety of information about each property. Then, using your algorithm, you will choose 200 houses to invest in out of about 2000 houses on the market at the moment.


<b>Learning objectives</b>
 
<ol type="i">
  <li>Using different data mining algorithms for prediction.</li>
  <li>Dealing with large data sets</li>
  <li>Tuning data mining algorithms</li>
  <li>Interpreting data mining algorithms and deducing importance of variables</li>
  <li>Using results of data mining algorithms to make business decisions</li>
</ol>  
</div>

# Load data

There are two sets of data, i) training data that has the actual prices ii) out of sample data that has the asking prices. Load both data sets. 

Make sure you understand what information each column contains. Note that not all information provided might be useful in predicting house prices, but do not make any assumptions before you decide what information you use in your prediction algorithms.

```{r read-investigate}
#read in the data

london_house_prices_2019_training<-read.csv("training_data_assignment_with_prices.csv")
london_house_prices_2019_out_of_sample<-read.csv("test_data_assignment.csv")



#fix data types in both data sets

#fix dates
london_house_prices_2019_training <- london_house_prices_2019_training %>% mutate(date=as.Date(date))
london_house_prices_2019_out_of_sample<-london_house_prices_2019_out_of_sample %>% mutate(date=as.Date(date))
#change characters to factors
london_house_prices_2019_training <- london_house_prices_2019_training %>% mutate_if(is.character,as.factor)
london_house_prices_2019_out_of_sample<-london_house_prices_2019_out_of_sample %>% mutate_if(is.character,as.factor)

#take a quick look at what's in the data
str(london_house_prices_2019_training)
str(london_house_prices_2019_out_of_sample)



```


```{r split the price data to training and testing}
set.seed(2999)
#let's do the initial split
train_test_split <- initial_split(london_house_prices_2019_training, prop = 0.75) #training set contains 75% of the data
# Create the training dataset
train_data <- training(train_test_split)
test_data <- testing(train_test_split)
```


# Visualize data 

Visualize and examine the data. What plots could be useful here? What do you learn from these visualizations?

```{r visualize Median daily pricing, fig.width= 10}
plotData <- london_house_prices_2019_training %>% 
         group_by(date) %>% 
         summarise(medianPrice = median(price))
ggplot(plotData, aes(x = date, y = medianPrice)) +
  geom_line(colour = "steelblue") +
  geom_point() +
  geom_smooth() +
  scale_y_continuous(labels = number) +
  theme_bw() +
  labs(x = "Date", y = "Daily Median Sales Price in GBP")
```

*There seems to be a slight seasonality in daily meadian house prices, with the summer being higher. However, this increase does not seem too drastic.*


```{r visualize Density, fig.width= 10}
ggplot(london_house_prices_2019_training, aes(x = price)) +
  geom_density(color = "steelblue") +
  scale_y_continuous(labels = percent) +
  scale_x_continuous(labels = number) +
  theme_bw() +
  labs(x = "Price in GBP", y = "Distribution")

```

*House prices are heaviliy skewed to the right. This indicates that there are large outlying values. We should assess the impact of this on our models.*

Estimate a correlation table between prices and other continuous variables. What do you glean from the correlation table?

*A first idea about potential relations in the data as well as potential drivers of our dependent price variable.*

```{r, correlation table, warning=FALSE, message=FALSE, fig.width=8}

# produce a correlation table using GGally::ggcorr()
# this takes a while to plot
london_house_prices_2019_training %>% 
  select(-ID) %>% #keep Y variable last
  ggcorr(method = c("pairwise", "pearson"), layout.exp = 2,label_round=2, label = TRUE,label_size = 2,hjust = 1,nbreaks = 5,size = 2) +
  theme_bw()

```

# Fit a linear regression model

```{r LR model}
#Define control variables
lrControl <- trainControl (
    method="cv",
    number=5,
    verboseIter=FALSE) #by setting this to FALSE the model will not report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation
model1_lm<-train(
    price ~  district*total_floor_area + london_zone*number_habitable_rooms + co2_emissions_potential + distance_to_station +water_company+property_type+latitude+ longitude + average_income + property_type,
    train_data,
   method = "lm",
    trControl = lrControl
   )

# summary of the results
summary(model1_lm)
```

*We have also seen how there are large outlying values in our response price variable.*

```{r LR log issue, fig.width = 5}
#untransformed price variable
plot(lm(price ~  district*total_floor_area + london_zone*number_habitable_rooms + co2_emissions_potential + distance_to_station +water_company+property_type+latitude+ longitude + average_income + property_type,
    train_data))
#transformed price variable
plot(lm(log(price) ~  district*total_floor_area + london_zone*number_habitable_rooms + co2_emissions_potential + distance_to_station +water_company+property_type+latitude+ longitude + average_income + property_type,
    train_data))
```

*In the above Plots, we can identify how residuals at extreme quartile ends diverge from normality. Let us therefore see how the performance of our linear regression changes when we do a log transformation of price.*

```{r LR logged model}
#building log transformed price linear regression
model1_lm_log<-train(
    log(price) ~  district*total_floor_area + london_zone*number_habitable_rooms + co2_emissions_potential + distance_to_station +water_company+property_type+latitude+ longitude + average_income + property_type,
    train_data,
   method = "lm",
    trControl = lrControl
   )

summary(model1_lm_log)
```

## Predict the values in testing and out of sample data

Below I use the predict function to test the performance of the model in testing data and summarize the performance of the linear regression model. How can you measure the quality of your predictions?

*We can assess the performance through 1) the fit of the model obtained through cross validation and 2) through out of sample testing and aiming for RMSE minimisation.*

```{r oos linear regression}

#results of untransformed price variable
lrPredictions <- predict(model1_lm,test_data)

lr_results<-data.frame(  RMSE = RMSE(lrPredictions, test_data$price), 
                            Rsquare = R2(lrPredictions, test_data$price))
lr_results 
```

```{r oos logged linear regression}

#results of transformed price variable

loggedPredictions <- exp(predict(model1_lm_log,test_data))

logged_lr_results<-data.frame(  RMSE = RMSE(loggedPredictions, test_data$price), 
                            Rsquare = R2(loggedPredictions, test_data$price))
logged_lr_results 
```

*As we can see, the log transformed model performs significantly worse, especially in out of sample testing. We therefore decide to stay with untransformed price as a response variable.* 

```{r variable Importance, fig.height=12, fig.width=12}
# we can check variable importance as well
importance <- varImp(model1_lm, scale=TRUE)
plot(importance)
```

```{r examine district prices, fig.height=12, fig.width=12}

train_data %>% 
  group_by(district) %>% 
  summarise(medianPrice = median(price)) %>% 
  ggplot(aes(x = medianPrice, y = fct_reorder(district, medianPrice))) +
    geom_col(fill = "steelblue") +
    theme_classic()

```

*Comparing the above plot with the plot on variable importance, we can see that interaction variables are especially important between floor area and districts that have high median prices in housing. This lets us to hypothesise that floor area has specifically high leverage on a houses valuation when it is located in an upscale area.*

# Fit a tree model

```{r tree model, fig.height=12, fig.width=12}
modelLookup("rpart") #To mitigate decision trees’ shortcomings, the introduction of a complexity parameter cp and an associated cost function penalising a tree’s size can be useful. In the case at hand, an array of values for cp are iterated/tuned through and optimal values are found by cross validating RMSEs. As trees run the risk of overfitting as their size increases, this pruning measure allows for optimising variance.

treeControl <- trainControl(
    method="cv",
    number=5,
    verboseIter=FALSE)

treeGrid <- expand.grid(cp = seq(0.0000, 0.001,0.00001))

model2_tree <- train(
    price ~ district*total_floor_area + london_zone + number_habitable_rooms + co2_emissions_potential + distance_to_station +water_company+property_type+latitude+ longitude + average_income,
  train_data,
  method = "rpart",
  trControl = treeControl,
  tuneLength=10,
  tuneGrid = treeGrid
    )

plot(model2_tree)
treeImportance <- varImp(model2_tree, scale=TRUE)
plot(treeImportance)
```

*The plot on the complexity parameter against the cross validated RMSE performance nicely indicates the optimally tuned penalising parameter. It is furthermore interesting to see how variable importance gets is different to , which can possibly be attributed to trees susceptibility to over emphasise variables with large outlying values.*

```{r oos trees}
# We can predict the testing values

treePredictions <- predict(model2_tree,test_data)

tree_results<-data.frame(  RMSE = RMSE(treePredictions, test_data$price), 
                            Rsquare = R2(treePredictions, test_data$price))
tree_results                         
```

*One can argue that the poor OOS performance of the Regression Trees is caused by the algorithm’s tendency towards high variance. Applying this notion to the problem of predicting prices, it is likely that the presence of large outlying values in training sets heavily influences predictions to extreme degrees. Thus, this model is not considered viable for further analyses, especially because random forests represent an improvement to simpler regression trees.*

# Other algorithms

Use at least two other algorithms to predict prices. Don't forget to tune the parameters of these algorithms. And then compare the performances of your algorithms to linear regression and trees.

```{r support vector machines}
modelLookup("svmRadial")

# Tuning of C and Sigma is not done in a pre-specified grid fashion, but is left over to the algorithm to get a sense for the outcomes in a broader and less constrained way.

svmControl <- trainControl(method="cv", number=5, verboseIter = FALSE) #again, train controlling is done through cross validation

model3_svm <- train(
    price ~ total_floor_area + district + london_zone + number_habitable_rooms + co2_emissions_potential + distance_to_station +water_company+property_type+latitude+ longitude + average_income,
  train_data,
  method = "svmRadial",
  trControl = svmControl,
  tuneLength = 10)

svmImportance <- varImp(model3_svm, scale=TRUE)
plot(svmImportance)

```

```{r oos svm}

# lets also assess the out of sample performance of support vector regression
svmPredictions <- predict(model3_svm,test_data)

svm_results<-data.frame(  RMSE = RMSE(svmPredictions, test_data$price), 
                            Rsquare = R2(svmPredictions, test_data$price))
svm_results   
```

```{r random forest}
#next, we construct random forests

modelLookup("ranger")
rfControl <- trainControl(verboseIter = FALSE, method = "cv", number = 5)

# Tuning on mtry (number of predictors randomly sampled at each split), splitrule and min.node.size is not done in a pre-specified grid fashion, but is left over to the algorithm to get a sense for the outcomes in a broader and less constrained way.

model4_randomForests <- train(
    price ~ total_floor_area + district + london_zone + number_habitable_rooms + co2_emissions_potential + distance_to_station +water_company+property_type+latitude+ longitude + average_income,
    na.action = na.omit,
    train_data,
    method = "ranger",
    trControl = rfControl
)
```

```{r oos rf}
#we also assess the out of sample performance of the constructed trees
rfPredictions <- predict(model4_randomForests,test_data)

rf_results<-data.frame(  RMSE = RMSE(rfPredictions, test_data$price), 
                            Rsquare = R2(rfPredictions, test_data$price))
rf_results

```

```{r gradient boosting machine, warning=FALSE,message=FALSE,error=FALSE, echo=FALSE}
#as a last algorithm, we engage in gradient boosting models

#lets tune the characteristics of the built trees
# tuning is mostly done to mitigate overfitting/variance by adjusting multiple parameters, especially through limiting n.tree (the number of trees/iterations) and interaction.depth (the number of nodes). At the heart of GBM is shrinkage, regularising the impact of additionally added trees to limit susceptibility to overfitting
gbmGrid <- expand.grid(interaction.depth=c(1, 3, 5), n.trees = (0:50)*50,shrinkage=c(0.01, 0.001), n.minobsinnode=10)

metricToMinimise <- "RMSE"
trainControl <- trainControl(method="cv", number=5, verboseIter = FALSE)

model5_gbm <- train(
    price ~ total_floor_area + district + london_zone + number_habitable_rooms + co2_emissions_potential + distance_to_station +water_company+property_type+latitude+ longitude + average_income + property_type, 
    train_data,
    distribution="gaussian",
    method="gbm",
    trControl=trainControl,
    tuneGrid=gbmGrid,
    metric=metricToMinimise, 
    bag.fraction=0.75)                  
```

```{r gbm oos}
#out of sample testing of gbm
gbmPredictions <- predict(model5_gbm,test_data)

gbm_results<-data.frame( RMSE = RMSE(gbmPredictions, test_data$price), 
                            Rsquare = R2(gbmPredictions, test_data$price))
gbm_results

```

```{r result summaries}
models <- c("Linear Regression", "Regression Trees", "Support Vector Machines", "Random Forests", "Gradient Boosting Methods")
resultSummary = cbind(models, rbind(lr_results, tree_results, svm_results, rf_results, gbm_results))
resultSummary
```

*Our constructed Support Vector Machines seem to be most accurate in out of sample testing. We will focus on the last three models for ensembling.*

# Stacking

```{r,warning=FALSE,  message=FALSE }
stackingControl <- trainControl(method="cv", number=5, verboseIter = FALSE)

#first, lets look at the best tunes from each of the single underlying models such that we can take the chosen parameters for stacking
model3_svm$bestTune
model4_randomForests$bestTune
model5_gbm$bestTune

modelList <- caretList(
    price ~ total_floor_area + district + london_zone + number_habitable_rooms + co2_emissions_potential + distance_to_station +water_company+property_type+latitude+ longitude + average_income + property_type,
  train_data,
  trControl = stackingControl,
  tuneList = list(
    #we create a model list with the best tunes of each model
    svm = caretModelSpec(method = "svmRadial", tuneGrid = data.frame(sigma = 0.01226624	, C = 32)),
    ranger = caretModelSpec(method = "ranger", tuneGrid = data.frame(mtry=47,splitrule="extratrees",min.node.size=5)),
    gbm = caretModelSpec(method = "gbm", tuneGrid = data.frame(n.trees = 2500, interaction.depth = 5, shrinkage = 0.01, n.minobsinnode = 10))
  )
)

model6_stacking <- caretStack(
  modelList,
  method = "glm", #we use logistic regression as a combiner algorithm to balance between underlying models
  metric = "RMSE",
  trControl = stackingControl
)

stackingPredictions <- predict(model6_stacking,test_data)

stacking_results<-data.frame(RMSE = RMSE(stackingPredictions, test_data$price), 
                            Rsquare = R2(stackingPredictions, test_data$price))
stacking_results
```
*As we can see, this one-time oos testing has given us a slighlty worse performance than support vector machines. However, it is likely to assume that across different tests, stacking will be more consistent and robust due to mitigated overfitting.* 


```{r RMSEs}

#lets look at the confidence intervals for RMSEs of each underlying model
resamples <- resamples(modelList)

dotplot(resamples, metric = "RMSE")
modelCor(resamples)
```
*The above plot aids in determining the robustness of our out of sample performance result. As we can see, the width of each confidence interval across the underlying algorithms are rather similar. This hints at a well balanced choice of models for the ensemble algorithm.*
*As a next step, lets assess what happens when we include linear regression in the stacking algorithm. Maybe this has the potential to enhance the final result, since it might counteract the tendency to overfit of the existing three models.*

```{r stacking oos including linear reg}

withLRmodelList <- caretList(
    price ~ total_floor_area + district + london_zone + number_habitable_rooms + co2_emissions_potential + distance_to_station +water_company+property_type+latitude+ longitude + average_income + property_type,
  train_data,
  trControl = stackingControl,
  tuneList = list(
    lr = caretModelSpec(method = "lm"),
    svm = caretModelSpec(method = "svmRadial", tuneGrid = data.frame(sigma = 0.01226624	, C = 32)),
    ranger = caretModelSpec(method = "ranger", tuneGrid = data.frame(mtry=47,splitrule="extratrees",min.node.size=5)),
    gbm = caretModelSpec(method = "gbm", tuneGrid = data.frame(n.trees = 2500, interaction.depth = 5, shrinkage = 0.01, n.minobsinnode = 10))
  ))
  
withLRmodel7_stacking <- caretStack(
  withLRmodelList,
  method = "glm",
  metric = "RMSE",
  trControl = stackingControl
)
withLRstackingPredictions <- predict(withLRmodel7_stacking,test_data)

withLRstacking_results<-data.frame(RMSE = RMSE(withLRstackingPredictions, test_data$price), 
                            Rsquare = R2(withLRstackingPredictions, test_data$price))
withLRstacking_results
```

*As we can see, the introduction of linear regression to our stacking ensemble actually reduces the out of sample performance slightly. Lets not consider it for our final model.*

# Pick investments

*We want to maximiize the average percentage return/profit on invested houses. This means that we have to predict actual prices/valuations as accurately as possible. We thus choose the stacking model (excluding Linear Regression) as it was showing the best out of sample performance.*

```{r,warning=FALSE,  message=FALSE }
oos<-london_house_prices_2019_out_of_sample

#predict the value of houses
oos$predict <- predict(model6_stacking,oos)
#Choose the ones you want to invest here
#Make sure you choose exactly 200 of them

mutOos <- oos %>% 
  mutate(absProfit = predict - asking_price, roi = (predict - asking_price)/asking_price)

#output your choices. Change the name of the file to your "lastname_firstname.csv"
write.csv(mutOos,"my_submission.csv")

```
